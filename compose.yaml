name: arta

services:
    bot:
        build:
            context: .
            dockerfile: Dockerfile
        image: mayusha256/arta:latest
        container_name: bot
        restart: unless-stopped
        environment:
            - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
        depends_on:
            llama:
                condition: service_healthy

    qdrant:
        image: qdrant/qdrant:v1.16-gpu-nvidia
        restart: unless-stopped
        container_name: qdrant
        ports:
            - 6333:6333
            - 6334:6334
        expose:
            - 6333
            - 6334
            - 6335
        volumes:
            - qdrant:/qdrant/storage
        environment:
            - QDRANT__GPU__INDEXING=1
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]

    llama:
        image: ghcr.io/ggml-org/llama.cpp:server-cuda
        container_name: llama
        ports:
            - "8080:8080"
        volumes:
            - models:/models
        environment:
            - LLAMA_CACHE=/models
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]
        command: >
            -hf unsloth/gpt-oss-20b-GGUF:Q4_K_M
            --host 0.0.0.0
            --port 8080
            --n-gpu-layers 99
            --threads 16
            --threads-batch 8
            --ctx-size 8192
            --batch-size 4096
            --ubatch-size 1024
            --keep 512
            --no-mmap
            --mlock
            --sleep-idle-seconds 120
        restart: unless-stopped
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 3
            start_period: 40s

volumes:
    qdrant:
        driver: local
    models:
        driver: local
